{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ7XRvbkOzvc0zHoX6VNzE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sesmael/Real-Time-ML-/blob/main/Homework5_problem1_and_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqyJqXcp-Cl_",
        "outputId": "f761224d-1308-4c71-c1cf-aa354efe6f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sequence Length: 10 ---\n",
            "\n",
            "Training Transformer for sequence length 10:\n",
            "Epoch 1/20 | Train Loss: 2.9575 | Val Acc: 0.2353\n",
            "Epoch 5/20 | Train Loss: 1.9924 | Val Acc: 0.2983\n",
            "Epoch 10/20 | Train Loss: 1.3690 | Val Acc: 0.3361\n",
            "Epoch 15/20 | Train Loss: 0.9028 | Val Acc: 0.3845\n",
            "Epoch 20/20 | Train Loss: 0.6365 | Val Acc: 0.3971\n",
            "Final Transformer - Train Loss: 0.6365, Val Acc: 0.3971, Time: 3.77 sec\n",
            "\n",
            "Training LSTM for sequence length 10:\n",
            "Epoch 1/20 | Train Loss: 3.1280 | Val Acc: 0.2143\n",
            "Epoch 5/20 | Train Loss: 1.8424 | Val Acc: 0.4307\n",
            "Epoch 10/20 | Train Loss: 1.1317 | Val Acc: 0.4559\n",
            "Epoch 15/20 | Train Loss: 0.6082 | Val Acc: 0.4643\n",
            "Epoch 20/20 | Train Loss: 0.2738 | Val Acc: 0.4265\n",
            "Final LSTM - Train Loss: 0.2738, Val Acc: 0.4265, Time: 1.93 sec\n",
            "\n",
            "Training LSTM+Attn for sequence length 10:\n",
            "Epoch 1/20 | Train Loss: 3.1999 | Val Acc: 0.1345\n",
            "Epoch 5/20 | Train Loss: 2.2872 | Val Acc: 0.3067\n",
            "Epoch 10/20 | Train Loss: 1.1818 | Val Acc: 0.3908\n",
            "Epoch 15/20 | Train Loss: 0.4208 | Val Acc: 0.3782\n",
            "Epoch 20/20 | Train Loss: 0.1978 | Val Acc: 0.3824\n",
            "Final LSTM+Attn - Train Loss: 0.1978, Val Acc: 0.3824, Time: 2.97 sec\n",
            "\n",
            "Training EncDec+Attn for sequence length 10:\n",
            "Epoch 1/20 | Train Loss: 3.2049 | Val Acc: 0.1345\n",
            "Epoch 5/20 | Train Loss: 2.3030 | Val Acc: 0.3046\n",
            "Epoch 10/20 | Train Loss: 1.1801 | Val Acc: 0.3992\n",
            "Epoch 15/20 | Train Loss: 0.3770 | Val Acc: 0.3866\n",
            "Epoch 20/20 | Train Loss: 0.1779 | Val Acc: 0.4076\n",
            "Final EncDec+Attn - Train Loss: 0.1779, Val Acc: 0.4076, Time: 3.52 sec\n",
            "\n",
            "--- Sequence Length: 20 ---\n",
            "\n",
            "Training Transformer for sequence length 20:\n",
            "Epoch 1/20 | Train Loss: 2.9430 | Val Acc: 0.2278\n",
            "Epoch 5/20 | Train Loss: 2.2101 | Val Acc: 0.2722\n",
            "Epoch 10/20 | Train Loss: 1.8060 | Val Acc: 0.2595\n",
            "Epoch 15/20 | Train Loss: 1.3112 | Val Acc: 0.2827\n",
            "Epoch 20/20 | Train Loss: 0.9075 | Val Acc: 0.2869\n",
            "Final Transformer - Train Loss: 0.9075, Val Acc: 0.2869, Time: 3.81 sec\n",
            "\n",
            "Training LSTM for sequence length 20:\n",
            "Epoch 1/20 | Train Loss: 3.1307 | Val Acc: 0.2152\n",
            "Epoch 5/20 | Train Loss: 1.9080 | Val Acc: 0.3903\n",
            "Epoch 10/20 | Train Loss: 1.2390 | Val Acc: 0.4494\n",
            "Epoch 15/20 | Train Loss: 0.7450 | Val Acc: 0.4536\n",
            "Epoch 20/20 | Train Loss: 0.3941 | Val Acc: 0.4536\n",
            "Final LSTM - Train Loss: 0.3941, Val Acc: 0.4536, Time: 2.06 sec\n",
            "\n",
            "Training LSTM+Attn for sequence length 20:\n",
            "Epoch 1/20 | Train Loss: 3.2029 | Val Acc: 0.1350\n",
            "Epoch 5/20 | Train Loss: 2.3809 | Val Acc: 0.2911\n",
            "Epoch 10/20 | Train Loss: 1.4327 | Val Acc: 0.3692\n",
            "Epoch 15/20 | Train Loss: 0.5600 | Val Acc: 0.4051\n",
            "Epoch 20/20 | Train Loss: 0.1586 | Val Acc: 0.3924\n",
            "Final LSTM+Attn - Train Loss: 0.1586, Val Acc: 0.3924, Time: 3.40 sec\n",
            "\n",
            "Training EncDec+Attn for sequence length 20:\n",
            "Epoch 1/20 | Train Loss: 3.2005 | Val Acc: 0.0907\n",
            "Epoch 5/20 | Train Loss: 2.4966 | Val Acc: 0.2574\n",
            "Epoch 10/20 | Train Loss: 1.4118 | Val Acc: 0.3755\n",
            "Epoch 15/20 | Train Loss: 0.5484 | Val Acc: 0.3776\n",
            "Epoch 20/20 | Train Loss: 0.1357 | Val Acc: 0.3861\n",
            "Final EncDec+Attn - Train Loss: 0.1357, Val Acc: 0.3861, Time: 3.43 sec\n",
            "\n",
            "--- Sequence Length: 30 ---\n",
            "\n",
            "Training Transformer for sequence length 30:\n",
            "Epoch 1/20 | Train Loss: 2.9152 | Val Acc: 0.2097\n",
            "Epoch 5/20 | Train Loss: 2.2558 | Val Acc: 0.2352\n",
            "Epoch 10/20 | Train Loss: 1.9422 | Val Acc: 0.2712\n",
            "Epoch 15/20 | Train Loss: 1.4215 | Val Acc: 0.3305\n",
            "Epoch 20/20 | Train Loss: 0.9675 | Val Acc: 0.3178\n",
            "Final Transformer - Train Loss: 0.9675, Val Acc: 0.3178, Time: 3.77 sec\n",
            "\n",
            "Training LSTM for sequence length 30:\n",
            "Epoch 1/20 | Train Loss: 3.0870 | Val Acc: 0.2288\n",
            "Epoch 5/20 | Train Loss: 1.8855 | Val Acc: 0.3856\n",
            "Epoch 10/20 | Train Loss: 1.2036 | Val Acc: 0.4428\n",
            "Epoch 15/20 | Train Loss: 0.7100 | Val Acc: 0.4576\n",
            "Epoch 20/20 | Train Loss: 0.3748 | Val Acc: 0.4640\n",
            "Final LSTM - Train Loss: 0.3748, Val Acc: 0.4640, Time: 2.30 sec\n",
            "\n",
            "Training LSTM+Attn for sequence length 30:\n",
            "Epoch 1/20 | Train Loss: 3.1744 | Val Acc: 0.0720\n",
            "Epoch 5/20 | Train Loss: 2.4675 | Val Acc: 0.2352\n",
            "Epoch 10/20 | Train Loss: 1.3589 | Val Acc: 0.3856\n",
            "Epoch 15/20 | Train Loss: 0.6237 | Val Acc: 0.3814\n",
            "Epoch 20/20 | Train Loss: 0.1626 | Val Acc: 0.3708\n",
            "Final LSTM+Attn - Train Loss: 0.1626, Val Acc: 0.3708, Time: 3.59 sec\n",
            "\n",
            "Training EncDec+Attn for sequence length 30:\n",
            "Epoch 1/20 | Train Loss: 3.1940 | Val Acc: 0.0911\n",
            "Epoch 5/20 | Train Loss: 2.5920 | Val Acc: 0.2203\n",
            "Epoch 10/20 | Train Loss: 1.6111 | Val Acc: 0.3623\n",
            "Epoch 15/20 | Train Loss: 0.6564 | Val Acc: 0.4047\n",
            "Epoch 20/20 | Train Loss: 0.2503 | Val Acc: 0.3814\n",
            "Final EncDec+Attn - Train Loss: 0.2503, Val Acc: 0.3814, Time: 3.60 sec\n",
            "\n",
            "\n",
            "--- Overall Comparison of Models ---\n",
            "\n",
            "Sequence Length: 10\n",
            "Transformer: Train Loss = 0.6365, Val Acc = 0.3971, Time = 3.77 sec\n",
            "LSTM: Train Loss = 0.2738, Val Acc = 0.4265, Time = 1.93 sec\n",
            "LSTM+Attn: Train Loss = 0.1978, Val Acc = 0.3824, Time = 2.97 sec\n",
            "EncDec+Attn: Train Loss = 0.1779, Val Acc = 0.4076, Time = 3.52 sec\n",
            "\n",
            "Sequence Length: 20\n",
            "Transformer: Train Loss = 0.9075, Val Acc = 0.2869, Time = 3.81 sec\n",
            "LSTM: Train Loss = 0.3941, Val Acc = 0.4536, Time = 2.06 sec\n",
            "LSTM+Attn: Train Loss = 0.1586, Val Acc = 0.3924, Time = 3.40 sec\n",
            "EncDec+Attn: Train Loss = 0.1357, Val Acc = 0.3861, Time = 3.43 sec\n",
            "\n",
            "Sequence Length: 30\n",
            "Transformer: Train Loss = 0.9675, Val Acc = 0.3178, Time = 3.77 sec\n",
            "LSTM: Train Loss = 0.3748, Val Acc = 0.4640, Time = 2.30 sec\n",
            "LSTM+Attn: Train Loss = 0.1626, Val Acc = 0.3708, Time = 3.59 sec\n",
            "EncDec+Attn: Train Loss = 0.2503, Val Acc = 0.3814, Time = 3.60 sec\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# Full English-to-French Dataset (for reference)\n",
        "# ---------------------------\n",
        "english_to_french = [\n",
        "    (\"I am cold\", \"J'ai froid\"),\n",
        "    (\"You are tired\", \"Tu es fatigué\"),\n",
        "    (\"He is hungry\", \"Il a faim\"),\n",
        "    (\"She is happy\", \"Elle est heureuse\"),\n",
        "    (\"We are friends\", \"Nous sommes amis\"),\n",
        "    (\"They are students\", \"Ils sont étudiants\"),\n",
        "    (\"The cat is sleeping\", \"Le chat dort\"),\n",
        "    (\"The sun is shining\", \"Le soleil brille\"),\n",
        "    (\"We love music\", \"Nous aimons la musique\"),\n",
        "    (\"She speaks French fluently\", \"Elle parle français couramment\"),\n",
        "    (\"He enjoys reading books\", \"Il aime lire des livres\"),\n",
        "    (\"They play soccer every weekend\", \"Ils jouent au football chaque week-end\"),\n",
        "    (\"The movie starts at 7 PM\", \"Le film commence à 19 heures\"),\n",
        "    (\"She wears a red dress\", \"Elle porte une robe rouge\"),\n",
        "    (\"We cook dinner together\", \"Nous cuisinons le dîner ensemble\"),\n",
        "    (\"He drives a blue car\", \"Il conduit une voiture bleue\"),\n",
        "    (\"They visit museums often\", \"Ils visitent souvent des musées\"),\n",
        "    (\"The restaurant serves delicious food\", \"Le restaurant sert une délicieuse cuisine\"),\n",
        "    (\"She studies mathematics at university\", \"Elle étudie les mathématiques à l'université\"),\n",
        "    (\"We watch movies on Fridays\", \"Nous regardons des films le vendredi\"),\n",
        "    (\"He listens to music while jogging\", \"Il écoute de la musique en faisant du jogging\"),\n",
        "    (\"They travel around the world\", \"Ils voyagent autour du monde\"),\n",
        "    (\"The book is on the table\", \"Le livre est sur la table\"),\n",
        "    (\"She dances gracefully\", \"Elle danse avec grâce\"),\n",
        "    (\"We celebrate birthdays with cake\", \"Nous célébrons les anniversaires avec un gâteau\"),\n",
        "    (\"He works hard every day\", \"Il travaille dur tous les jours\"),\n",
        "    (\"They speak different languages\", \"Ils parlent différentes langues\"),\n",
        "    (\"The flowers bloom in spring\", \"Les fleurs fleurissent au printemps\"),\n",
        "    (\"She writes poetry in her free time\", \"Elle écrit de la poésie pendant son temps libre\"),\n",
        "    (\"We learn something new every day\", \"Nous apprenons quelque chose de nouveau chaque jour\"),\n",
        "    (\"The dog barks loudly\", \"Le chien aboie bruyamment\"),\n",
        "    (\"He sings beautifully\", \"Il chante magnifiquement\"),\n",
        "    (\"They swim in the pool\", \"Ils nagent dans la piscine\"),\n",
        "    (\"The birds chirp in the morning\", \"Les oiseaux gazouillent le matin\"),\n",
        "    (\"She teaches English at school\", \"Elle enseigne l'anglais à l'école\"),\n",
        "    (\"We eat breakfast together\", \"Nous prenons le petit déjeuner ensemble\"),\n",
        "    (\"He paints landscapes\", \"Il peint des paysages\"),\n",
        "    (\"They laugh at the joke\", \"Ils rient de la blague\"),\n",
        "    (\"The clock ticks loudly\", \"L'horloge tic-tac bruyamment\"),\n",
        "    (\"She runs in the park\", \"Elle court dans le parc\"),\n",
        "    (\"We travel by train\", \"Nous voyageons en train\"),\n",
        "    (\"He writes a letter\", \"Il écrit une lettre\"),\n",
        "    (\"They read books at the library\", \"Ils lisent des livres à la bibliothèque\"),\n",
        "    (\"The baby cries\", \"Le bébé pleure\"),\n",
        "    (\"She studies hard for exams\", \"Elle étudie dur pour les examens\"),\n",
        "    (\"We plant flowers in the garden\", \"Nous plantons des fleurs dans le jardin\"),\n",
        "    (\"He fixes the car\", \"Il répare la voiture\"),\n",
        "    (\"They drink coffee in the morning\", \"Ils boivent du café le matin\"),\n",
        "    (\"The sun sets in the evening\", \"Le soleil se couche le soir\"),\n",
        "    (\"She dances at the party\", \"Elle danse à la fête\"),\n",
        "    (\"We play music at the concert\", \"Nous jouons de la musique au concert\"),\n",
        "    (\"He cooks dinner for his family\", \"Il cuisine le dîner pour sa famille\"),\n",
        "    (\"They study French grammar\", \"Ils étudient la grammaire française\"),\n",
        "    (\"The rain falls gently\", \"La pluie tombe doucement\"),\n",
        "    (\"She sings a song\", \"Elle chante une chanson\"),\n",
        "    (\"We watch a movie together\", \"Nous regardons un film ensemble\"),\n",
        "    (\"He sleeps deeply\", \"Il dort profondément\"),\n",
        "    (\"They travel to Paris\", \"Ils voyagent à Paris\"),\n",
        "    (\"The children play in the park\", \"Les enfants jouent dans le parc\"),\n",
        "    (\"She walks along the beach\", \"Elle se promène le long de la plage\"),\n",
        "    (\"We talk on the phone\", \"Nous parlons au téléphone\"),\n",
        "    (\"He waits for the bus\", \"Il attend le bus\"),\n",
        "    (\"They visit the Eiffel Tower\", \"Ils visitent la tour Eiffel\"),\n",
        "    (\"The stars twinkle at night\", \"Les étoiles scintillent la nuit\"),\n",
        "    (\"She dreams of flying\", \"Elle rêve de voler\"),\n",
        "    (\"We work in the office\", \"Nous travaillons au bureau\"),\n",
        "    (\"He studies history\", \"Il étudie l'histoire\"),\n",
        "    (\"They listen to the radio\", \"Ils écoutent la radio\"),\n",
        "    (\"The wind blows gently\", \"Le vent souffle doucement\"),\n",
        "    (\"She swims in the ocean\", \"Elle nage dans l'océan\"),\n",
        "    (\"We dance at the wedding\", \"Nous dansons au mariage\"),\n",
        "    (\"He climbs the mountain\", \"Il gravit la montagne\"),\n",
        "    (\"They hike in the forest\", \"Ils font de la randonnée dans la forêt\"),\n",
        "    (\"The cat meows loudly\", \"Le chat miaule bruyamment\"),\n",
        "    (\"She paints a picture\", \"Elle peint un tableau\"),\n",
        "    (\"We build a sandcastle\", \"Nous construisons un château de sable\"),\n",
        "    (\"He sings in the choir\", \"Il chante dans le chœur\"),\n",
        "    (\"They ride bicycles\", \"Ils font du vélo\"),\n",
        "    (\"The coffee is hot\", \"Le café est chaud\"),\n",
        "    (\"She wears glasses\", \"Elle porte des lunettes\"),\n",
        "    (\"We visit our grandparents\", \"Nous rendons visite à nos grands-parents\"),\n",
        "    (\"He plays the guitar\", \"Il joue de la guitare\"),\n",
        "    (\"They go shopping\", \"Ils font du shopping\"),\n",
        "    (\"The teacher explains the lesson\", \"Le professeur explique la leçon\"),\n",
        "    (\"She takes the train to work\", \"Elle prend le train pour aller au travail\"),\n",
        "    (\"We bake cookies\", \"Nous faisons des biscuits\"),\n",
        "    (\"He washes his hands\", \"Il se lave les mains\"),\n",
        "    (\"They enjoy the sunset\", \"Ils apprécient le coucher du soleil\"),\n",
        "    (\"The river flows calmly\", \"La rivière coule calmement\"),\n",
        "    (\"She feeds the cat\", \"Elle nourrit le chat\"),\n",
        "    (\"We visit the museum\", \"Nous visitons le musée\"),\n",
        "    (\"He fixes his bicycle\", \"Il répare son vélo\"),\n",
        "    (\"They paint the walls\", \"Ils peignent les murs\"),\n",
        "    (\"The baby sleeps peacefully\", \"Le bébé dort paisiblement\"),\n",
        "    (\"She ties her shoelaces\", \"Elle attache ses lacets\"),\n",
        "    (\"We climb the stairs\", \"Nous montons les escaliers\"),\n",
        "    (\"He shaves in the morning\", \"Il se rase le matin\"),\n",
        "    (\"They set the table\", \"Ils mettent la table\"),\n",
        "    (\"The airplane takes off\", \"L'avion décolle\"),\n",
        "    (\"She waters the plants\", \"Elle arrose les plantes\"),\n",
        "    (\"We practice yoga\", \"Nous pratiquons le yoga\"),\n",
        "    (\"He turns off the light\", \"Il éteint la lumière\"),\n",
        "    (\"They play video games\", \"Ils jouent aux jeux vidéo\"),\n",
        "    (\"The soup smells delicious\", \"La soupe sent délicieusement bon\"),\n",
        "    (\"She locks the door\", \"Elle ferme la porte à clé\"),\n",
        "    (\"We enjoy a picnic\", \"Nous profitons d'un pique-nique\"),\n",
        "    (\"He checks his email\", \"Il vérifie ses emails\"),\n",
        "    (\"They go to the gym\", \"Ils vont à la salle de sport\"),\n",
        "    (\"The moon shines brightly\", \"La lune brille intensément\"),\n",
        "    (\"She catches the bus\", \"Elle attrape le bus\"),\n",
        "    (\"We greet our neighbors\", \"Nous saluons nos voisins\"),\n",
        "    (\"He combs his hair\", \"Il se peigne les cheveux\"),\n",
        "    (\"They wave goodbye\", \"Ils font un signe d'adieu\")\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Preparing the Next Character Prediction Dataset\n",
        "# ---------------------------\n",
        "sequence_text = (\n",
        "    \"Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting \"\n",
        "    \"the next character in a sequence of text based on the characters that precede it. This task is essential for various \"\n",
        "    \"applications, including text auto-completion, spell checking, and even in the development of sophisticated AI models capable \"\n",
        "    \"of generating human-like text. At its core, next character prediction relies on statistical models or deep learning algorithms \"\n",
        "    \"to analyze a given sequence of text and predict which character is most likely to follow. These predictions are based on patterns \"\n",
        "    \"and relationships learned from large datasets of text during the training phase of the model. One of the most popular approaches to \"\n",
        "    \"next character prediction involves the use of Recurrent Neural Networks (RNNs), and more specifically, a variant called Long \"\n",
        "    \"Short-Term Memory (LSTM) networks. RNNs are particularly well-suited for sequential data like text, as they can maintain \"\n",
        "    \"information in 'memory' about previous characters to inform the prediction of the next character. LSTM networks enhance this capability \"\n",
        "    \"by being able to remember long-term dependencies, making them even more effective for next character prediction tasks. \"\n",
        "    \"Training a model for next character prediction involves feeding it large amounts of text data, allowing it to learn the probability \"\n",
        "    \"of each character's appearance following a sequence of characters. During this training process, the model adjusts its parameters \"\n",
        "    \"to minimize the difference between its predictions and the actual outcomes, thus improving its predictive accuracy over time. \"\n",
        "    \"Once trained, the model can be used to predict the next character in a given piece of text by considering the sequence of characters \"\n",
        "    \"that precede it. This can enhance user experience in text editing software, improve efficiency in coding environments with auto-completion \"\n",
        "    \"features, and enable more natural interactions with AI-based chatbots and virtual assistants. In summary, next character prediction plays \"\n",
        "    \"a crucial role in enhancing the capabilities of various NLP applications, making text-based interactions more efficient, accurate, and human-like. \"\n",
        "    \"Through the use of advanced machine learning models like RNNs and LSTMs, next character prediction continues to evolve, opening new possibilities \"\n",
        "    \"for the future of text-based technology.\"\n",
        ")\n",
        "\n",
        "# Build a character vocabulary and map characters to indices\n",
        "vocab = sorted(set(sequence_text))\n",
        "vocab_size = len(vocab)\n",
        "char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "def text_to_indices(text):\n",
        "    return [char_to_idx[ch] for ch in text]\n",
        "\n",
        "data_indices = text_to_indices(sequence_text)\n",
        "\n",
        "def create_dataset(seq_len):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data_indices) - seq_len):\n",
        "        X.append(data_indices[i:i+seq_len])\n",
        "        y.append(data_indices[i+seq_len])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define the Models\n",
        "# ---------------------------\n",
        "# Transformer-based model for next character prediction\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, hidden_dim=256, num_layers=2, seq_len=20, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)           # (batch, seq_len, embed_dim)\n",
        "        x = x + self.pos_embedding       # add positional info\n",
        "        x = x.transpose(0, 1)            # (seq_len, batch, embed_dim)\n",
        "        x = self.transformer_encoder(x)\n",
        "        out = x[-1]                     # last time step\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "# Standard LSTM-based model for next character prediction\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1, dropout=0.0):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.lstm(x)\n",
        "        out = self.fc_out(output[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# LSTM with Attention (without a separate decoder) for next character prediction\n",
        "class LSTMWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1, dropout=0.0):\n",
        "        super(LSTMWithAttention, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)                    # (batch, seq_len, hidden_dim)\n",
        "        lstm_out = lstm_out.transpose(0, 1)             # (seq_len, batch, hidden_dim)\n",
        "        query = lstm_out[-1].unsqueeze(0)              # (1, batch, hidden_dim)\n",
        "        attn_output, _ = self.attention(query, lstm_out, lstm_out)\n",
        "        attn_output = attn_output.squeeze(0)           # (batch, hidden_dim)\n",
        "        out = self.fc_out(attn_output)\n",
        "        return out\n",
        "\n",
        "# Encoder-Decoder with Cross-Attention\n",
        "class EncoderDecoderWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1, dropout=0.0):\n",
        "        super(EncoderDecoderWithAttention, self).__init__()\n",
        "        # Shared embedding for both encoder and decoder\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Encoder: processes the input sequence.\n",
        "        self.encoder = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        # Decoder: a single-step LSTM decoder.\n",
        "        self.decoder = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        # Learned start token for the decoder input.\n",
        "        self.start_token = nn.Parameter(torch.randn(1, embed_dim))\n",
        "\n",
        "        # Cross-attention: decoder queries attend to encoder outputs.\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, dropout=dropout)\n",
        "\n",
        "        # Final prediction layer.\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Encode input sequence.\n",
        "        encoder_emb = self.embedding(x)                   # (batch, seq_len, embed_dim)\n",
        "        encoder_outputs, (hn, cn) = self.encoder(encoder_emb)  # (batch, seq_len, hidden_dim)\n",
        "\n",
        "        # Prepare decoder input: use learned start token for each batch.\n",
        "        decoder_input = self.start_token.expand(batch_size, 1, -1)  # (batch, 1, embed_dim)\n",
        "        decoder_output, _ = self.decoder(decoder_input, (hn, cn))    # (batch, 1, hidden_dim)\n",
        "\n",
        "        # Prepare for attention: query from decoder, keys and values from encoder outputs.\n",
        "        decoder_output = decoder_output.transpose(0, 1)       # (1, batch, hidden_dim)\n",
        "        encoder_outputs_t = encoder_outputs.transpose(0, 1)   # (seq_len, batch, hidden_dim)\n",
        "\n",
        "        attn_output, _ = self.attention(decoder_output, encoder_outputs_t, encoder_outputs_t)\n",
        "        attn_output = attn_output.squeeze(0)                  # (batch, hidden_dim)\n",
        "        output = self.fc_out(attn_output)                     # (batch, vocab_size)\n",
        "        return output\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Training and Evaluation Functions\n",
        "# ---------------------------\n",
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            correct += (predictions == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Experiment Setup and Training Loop\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "seq_lengths = [10, 20, 30]\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "for seq_len in seq_lengths:\n",
        "    print(f\"\\n--- Sequence Length: {seq_len} ---\")\n",
        "    X, y = create_dataset(seq_len)\n",
        "    split_idx = int(0.8 * len(X))\n",
        "    X_train, y_train = X[:split_idx], y[:split_idx]\n",
        "    X_val, y_val = X[split_idx:], y[split_idx:]\n",
        "\n",
        "    train_dataset = TextDataset(X_train, y_train)\n",
        "    val_dataset = TextDataset(X_val, y_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    models = {\n",
        "        \"Transformer\": TransformerModel(vocab_size, seq_len=seq_len).to(device),\n",
        "        \"LSTM\": LSTMModel(vocab_size).to(device),\n",
        "        \"LSTM+Attn\": LSTMWithAttention(vocab_size).to(device),\n",
        "        \"EncDec+Attn\": EncoderDecoderWithAttention(vocab_size).to(device)\n",
        "    }\n",
        "\n",
        "    final_results[seq_len] = {}\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        print(f\"\\nTraining {model_name} for sequence length {seq_len}:\")\n",
        "        start_time = time.time()\n",
        "        final_train_loss = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
        "            val_acc = evaluate_model(model, val_loader, device)\n",
        "            final_train_loss = train_loss\n",
        "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        final_val_acc = evaluate_model(model, val_loader, device)\n",
        "\n",
        "        final_results[seq_len][model_name] = {\n",
        "            \"Final_Train_Loss\": final_train_loss,\n",
        "            \"Final_Val_Accuracy\": final_val_acc,\n",
        "            \"Total_Time_sec\": total_time\n",
        "        }\n",
        "        print(f\"Final {model_name} - Train Loss: {final_train_loss:.4f}, Val Acc: {final_val_acc:.4f}, Time: {total_time:.2f} sec\")\n",
        "\n",
        "print(\"\\n\\n--- Overall Comparison of Models ---\")\n",
        "for seq_len, model_results in final_results.items():\n",
        "    print(f\"\\nSequence Length: {seq_len}\")\n",
        "    for model_name, metrics in model_results.items():\n",
        "        print(f\"{model_name}: Train Loss = {metrics['Final_Train_Loss']:.4f}, Val Acc = {metrics['Final_Val_Accuracy']:.4f}, Time = {metrics['Total_Time_sec']:.2f} sec\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# ---------------------------\n",
        "# Download and Process the Tiny Shakespeare Dataset\n",
        "# ---------------------------\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text  # The entire text data\n",
        "\n",
        "def process_data(sequence_length):\n",
        "    # Create a character mapping to integers\n",
        "    chars = sorted(list(set(text)))\n",
        "    char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "    int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    # Encode the text into integers\n",
        "    encoded_text = [char_to_int[ch] for ch in text]\n",
        "\n",
        "    # Create sequences and targets\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(encoded_text) - sequence_length):\n",
        "        seq = encoded_text[i:i + sequence_length]\n",
        "        target = encoded_text[i + sequence_length]\n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "    # Convert lists to PyTorch tensors\n",
        "    return torch.tensor(sequences, dtype=torch.long), torch.tensor(targets, dtype=torch.long), char_to_int, int_to_char\n",
        "\n",
        "# ---------------------------\n",
        "# Define the Dataset Class\n",
        "# ---------------------------\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "# ---------------------------\n",
        "# Define the Transformer Model for Next Character Prediction\n",
        "# ---------------------------\n",
        "class TransformerCharModel(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, embed_dim=128, num_layers=2, num_heads=2, hidden_dim=256, dropout=0.1):\n",
        "        \"\"\"\n",
        "        vocab_size: Number of unique characters\n",
        "        seq_len: Input sequence length\n",
        "        embed_dim: Dimension of character embeddings\n",
        "        num_layers: Number of transformer encoder layers\n",
        "        num_heads: Number of attention heads\n",
        "        hidden_dim: Hidden dimension of the feed-forward network\n",
        "        dropout: Dropout probability\n",
        "        \"\"\"\n",
        "        super(TransformerCharModel, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # Learned positional embeddings\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim,\n",
        "                                                   nhead=num_heads,\n",
        "                                                   dim_feedforward=hidden_dim,\n",
        "                                                   dropout=dropout,\n",
        "                                                   batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len)\n",
        "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
        "        x = x + self.pos_embedding  # add positional embeddings\n",
        "        x = self.transformer_encoder(x)  # (batch_size, seq_len, embed_dim)\n",
        "        # Use the output of the last time step for prediction\n",
        "        out = self.fc_out(x[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# ---------------------------\n",
        "# Define an RNN Baseline (LSTM) for Comparison\n",
        "# ---------------------------\n",
        "class LSTMCharModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1, dropout=0.0):\n",
        "        super(LSTMCharModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc_out(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# ---------------------------\n",
        "# Training and Evaluation Functions\n",
        "# ---------------------------\n",
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            correct += (predictions == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# ---------------------------\n",
        "# Experiment Loop\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameter grid for transformers\n",
        "layer_options = [1, 2, 4]\n",
        "head_options = [2, 4]\n",
        "# You might also want to adjust hidden_dim (feed-forward network size) and embed_dim.\n",
        "\n",
        "sequence_lengths = [20, 30]  # later we will try 50 as well\n",
        "num_epochs = 10  # set to desired number for experiments\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Dictionary to store experiment results\n",
        "experiment_results = {}\n",
        "\n",
        "for seq_len in sequence_lengths:\n",
        "    print(f\"\\n=== Sequence Length: {seq_len} ===\")\n",
        "    # Process data for current sequence length\n",
        "    sequences, targets, char_to_int, int_to_char = process_data(seq_len)\n",
        "    dataset = CharDataset(sequences, targets)\n",
        "    # Simple train-validation split (80/20)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    vocab_size = len(char_to_int)\n",
        "    experiment_results[seq_len] = {}\n",
        "\n",
        "    # Loop over transformer hyperparameters combinations\n",
        "    for num_layers in layer_options:\n",
        "        for num_heads in head_options:\n",
        "            config_name = f\"Transformer_layers{num_layers}_heads{num_heads}\"\n",
        "            print(f\"\\n--- {config_name} ---\")\n",
        "            model = TransformerCharModel(vocab_size, seq_len=seq_len,\n",
        "                                         embed_dim=128,\n",
        "                                         num_layers=num_layers,\n",
        "                                         num_heads=num_heads,\n",
        "                                         hidden_dim=256,\n",
        "                                         dropout=0.1).to(device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            start_time = time.time()\n",
        "            for epoch in range(num_epochs):\n",
        "                train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
        "                val_acc = evaluate_model(model, val_loader, device)\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "            elapsed_time = time.time() - start_time\n",
        "            final_val_acc = evaluate_model(model, val_loader, device)\n",
        "\n",
        "            # Save results for this configuration\n",
        "            experiment_results[seq_len][config_name] = {\n",
        "                \"Final_Train_Loss\": train_loss,\n",
        "                \"Final_Val_Accuracy\": final_val_acc,\n",
        "                \"Training_Time_sec\": elapsed_time,\n",
        "                \"Model_Size\": sum(p.numel() for p in model.parameters())\n",
        "            }\n",
        "\n",
        "    # Train and evaluate the RNN baseline (for comparison)\n",
        "    print(\"\\n--- RNN Baseline (LSTM) ---\")\n",
        "    rnn_model = LSTMCharModel(vocab_size, embed_dim=128, hidden_dim=256, num_layers=1, dropout=0.0).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_model(rnn_model, train_loader, criterion, optimizer, device)\n",
        "        val_acc = evaluate_model(rnn_model, val_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "    elapsed_time = time.time() - start_time\n",
        "    final_val_acc = evaluate_model(rnn_model, val_loader, device)\n",
        "\n",
        "    experiment_results[seq_len][\"RNN_Baseline\"] = {\n",
        "        \"Final_Train_Loss\": train_loss,\n",
        "        \"Final_Val_Accuracy\": final_val_acc,\n",
        "        \"Training_Time_sec\": elapsed_time,\n",
        "        \"Model_Size\": sum(p.numel() for p in rnn_model.parameters())\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Increase Sequence Length to 50 and Report Results\n",
        "# ---------------------------\n",
        "seq_len = 50\n",
        "print(f\"\\n=== Sequence Length: {seq_len} (Increased) ===\")\n",
        "sequences, targets, char_to_int, int_to_char = process_data(seq_len)\n",
        "dataset = CharDataset(sequences, targets)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "vocab_size = len(char_to_int)\n",
        "\n",
        "# Use one configuration (say, 2 layers and 2 heads) for illustration\n",
        "print(\"\\n--- Transformer (2 layers, 2 heads) with sequence length 50 ---\")\n",
        "model = TransformerCharModel(vocab_size, seq_len=seq_len,\n",
        "                             embed_dim=128,\n",
        "                             num_layers=2,\n",
        "                             num_heads=2,\n",
        "                             hidden_dim=256,\n",
        "                             dropout=0.1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
        "    val_acc = evaluate_model(model, val_loader, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "elapsed_time = time.time() - start_time\n",
        "final_val_acc = evaluate_model(model, val_loader, device)\n",
        "experiment_results[seq_len] = {\n",
        "    \"Transformer_2layers_2heads\": {\n",
        "        \"Final_Train_Loss\": train_loss,\n",
        "        \"Final_Val_Accuracy\": final_val_acc,\n",
        "        \"Training_Time_sec\": elapsed_time,\n",
        "        \"Model_Size\": sum(p.numel() for p in model.parameters())\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n\\n--- Experiment Results ---\")\n",
        "for seq_len, results in experiment_results.items():\n",
        "    print(f\"\\nSequence Length: {seq_len}\")\n",
        "    for config, metrics in results.items():\n",
        "        print(f\"{config}: Train Loss = {metrics['Final_Train_Loss']:.4f}, Val Acc = {metrics['Final_Val_Accuracy']:.4f}, \"\n",
        "              f\"Time = {metrics['Training_Time_sec']:.2f} sec, Model Size = {metrics['Model_Size']} parameters\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU7kaFi4GbXk",
        "outputId": "91dca51e-d0e8-4471-e45a-d3a81b74d8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sequence Length: 20 ===\n",
            "\n",
            "--- Transformer_layers1_heads2 ---\n",
            "Epoch 1/10: Train Loss=2.0604, Val Acc=0.4451\n",
            "Epoch 2/10: Train Loss=1.9102, Val Acc=0.4558\n",
            "Epoch 3/10: Train Loss=1.8828, Val Acc=0.4587\n",
            "Epoch 4/10: Train Loss=1.8711, Val Acc=0.4621\n",
            "Epoch 5/10: Train Loss=1.8628, Val Acc=0.4669\n",
            "Epoch 6/10: Train Loss=1.8585, Val Acc=0.4652\n",
            "Epoch 7/10: Train Loss=1.8561, Val Acc=0.4643\n",
            "Epoch 8/10: Train Loss=1.8503, Val Acc=0.4652\n",
            "Epoch 9/10: Train Loss=1.8492, Val Acc=0.4701\n",
            "Epoch 10/10: Train Loss=1.8478, Val Acc=0.4681\n",
            "\n",
            "--- Transformer_layers1_heads4 ---\n",
            "Epoch 1/10: Train Loss=2.0120, Val Acc=0.4656\n",
            "Epoch 2/10: Train Loss=1.8397, Val Acc=0.4760\n",
            "Epoch 3/10: Train Loss=1.8083, Val Acc=0.4805\n",
            "Epoch 4/10: Train Loss=1.7929, Val Acc=0.4816\n",
            "Epoch 5/10: Train Loss=1.7856, Val Acc=0.4887\n",
            "Epoch 6/10: Train Loss=1.7797, Val Acc=0.4851\n",
            "Epoch 7/10: Train Loss=1.7765, Val Acc=0.4893\n",
            "Epoch 8/10: Train Loss=1.7739, Val Acc=0.4906\n",
            "Epoch 9/10: Train Loss=1.7694, Val Acc=0.4903\n",
            "Epoch 10/10: Train Loss=1.7676, Val Acc=0.4920\n",
            "\n",
            "--- Transformer_layers2_heads2 ---\n",
            "Epoch 1/10: Train Loss=1.9952, Val Acc=0.4690\n",
            "Epoch 2/10: Train Loss=1.8143, Val Acc=0.4846\n",
            "Epoch 3/10: Train Loss=1.7744, Val Acc=0.4898\n",
            "Epoch 4/10: Train Loss=1.7531, Val Acc=0.4960\n",
            "Epoch 5/10: Train Loss=1.7375, Val Acc=0.4965\n",
            "Epoch 6/10: Train Loss=1.7275, Val Acc=0.4984\n",
            "Epoch 7/10: Train Loss=1.7206, Val Acc=0.5065\n",
            "Epoch 8/10: Train Loss=1.7128, Val Acc=0.5063\n",
            "Epoch 9/10: Train Loss=1.7063, Val Acc=0.5042\n",
            "Epoch 10/10: Train Loss=1.7007, Val Acc=0.5099\n",
            "\n",
            "--- Transformer_layers2_heads4 ---\n",
            "Epoch 1/10: Train Loss=1.9697, Val Acc=0.4722\n",
            "Epoch 2/10: Train Loss=1.7798, Val Acc=0.4907\n",
            "Epoch 3/10: Train Loss=1.7353, Val Acc=0.4980\n",
            "Epoch 4/10: Train Loss=1.7129, Val Acc=0.5041\n",
            "Epoch 5/10: Train Loss=1.6985, Val Acc=0.5068\n",
            "Epoch 6/10: Train Loss=1.6868, Val Acc=0.5089\n",
            "Epoch 7/10: Train Loss=1.6782, Val Acc=0.5113\n",
            "Epoch 8/10: Train Loss=1.6713, Val Acc=0.5139\n",
            "Epoch 9/10: Train Loss=1.6658, Val Acc=0.5164\n",
            "Epoch 10/10: Train Loss=1.6578, Val Acc=0.5160\n",
            "\n",
            "--- Transformer_layers4_heads2 ---\n",
            "Epoch 1/10: Train Loss=1.9826, Val Acc=0.4672\n",
            "Epoch 2/10: Train Loss=1.8008, Val Acc=0.4818\n",
            "Epoch 3/10: Train Loss=1.7580, Val Acc=0.4893\n",
            "Epoch 4/10: Train Loss=1.7301, Val Acc=0.4948\n",
            "Epoch 5/10: Train Loss=1.7098, Val Acc=0.5041\n",
            "Epoch 6/10: Train Loss=1.6933, Val Acc=0.5066\n",
            "Epoch 7/10: Train Loss=1.6795, Val Acc=0.5109\n",
            "Epoch 8/10: Train Loss=1.6663, Val Acc=0.5138\n",
            "Epoch 9/10: Train Loss=1.6581, Val Acc=0.5155\n",
            "Epoch 10/10: Train Loss=1.6486, Val Acc=0.5152\n",
            "\n",
            "--- Transformer_layers4_heads4 ---\n",
            "Epoch 1/10: Train Loss=1.9654, Val Acc=0.4759\n",
            "Epoch 2/10: Train Loss=1.7773, Val Acc=0.4905\n",
            "Epoch 3/10: Train Loss=1.7349, Val Acc=0.4988\n",
            "Epoch 4/10: Train Loss=1.7102, Val Acc=0.5032\n",
            "Epoch 5/10: Train Loss=1.6929, Val Acc=0.5109\n",
            "Epoch 6/10: Train Loss=1.6810, Val Acc=0.5087\n",
            "Epoch 7/10: Train Loss=1.6685, Val Acc=0.5145\n",
            "Epoch 8/10: Train Loss=1.6603, Val Acc=0.5108\n",
            "Epoch 9/10: Train Loss=1.6544, Val Acc=0.5154\n",
            "Epoch 10/10: Train Loss=1.6521, Val Acc=0.5163\n",
            "\n",
            "--- RNN Baseline (LSTM) ---\n",
            "Epoch 1/10: Train Loss=1.6681, Val Acc=0.5417\n",
            "Epoch 2/10: Train Loss=1.4518, Val Acc=0.5590\n",
            "Epoch 3/10: Train Loss=1.3976, Val Acc=0.5670\n",
            "Epoch 4/10: Train Loss=1.3659, Val Acc=0.5730\n",
            "Epoch 5/10: Train Loss=1.3457, Val Acc=0.5734\n",
            "Epoch 6/10: Train Loss=1.3304, Val Acc=0.5749\n",
            "Epoch 7/10: Train Loss=1.3181, Val Acc=0.5768\n",
            "Epoch 8/10: Train Loss=1.3099, Val Acc=0.5769\n",
            "Epoch 9/10: Train Loss=1.3022, Val Acc=0.5781\n",
            "Epoch 10/10: Train Loss=1.2956, Val Acc=0.5760\n",
            "\n",
            "=== Sequence Length: 30 ===\n",
            "\n",
            "--- Transformer_layers1_heads2 ---\n",
            "Epoch 1/10: Train Loss=2.0696, Val Acc=0.4545\n",
            "Epoch 2/10: Train Loss=1.9074, Val Acc=0.4632\n",
            "Epoch 3/10: Train Loss=1.8806, Val Acc=0.4708\n",
            "Epoch 4/10: Train Loss=1.8647, Val Acc=0.4714\n",
            "Epoch 5/10: Train Loss=1.8564, Val Acc=0.4735\n",
            "Epoch 6/10: Train Loss=1.8527, Val Acc=0.4733\n",
            "Epoch 7/10: Train Loss=1.8465, Val Acc=0.4766\n",
            "Epoch 8/10: Train Loss=1.8451, Val Acc=0.4788\n",
            "Epoch 9/10: Train Loss=1.8396, Val Acc=0.4800\n",
            "Epoch 10/10: Train Loss=1.8362, Val Acc=0.4813\n",
            "\n",
            "--- Transformer_layers1_heads4 ---\n",
            "Epoch 1/10: Train Loss=2.0091, Val Acc=0.4735\n",
            "Epoch 2/10: Train Loss=1.8361, Val Acc=0.4848\n",
            "Epoch 3/10: Train Loss=1.8046, Val Acc=0.4882\n",
            "Epoch 4/10: Train Loss=1.7886, Val Acc=0.4929\n",
            "Epoch 5/10: Train Loss=1.7790, Val Acc=0.4943\n",
            "Epoch 6/10: Train Loss=1.7732, Val Acc=0.4956\n",
            "Epoch 7/10: Train Loss=1.7692, Val Acc=0.4974\n",
            "Epoch 8/10: Train Loss=1.7666, Val Acc=0.4972\n",
            "Epoch 9/10: Train Loss=1.7636, Val Acc=0.4977\n",
            "Epoch 10/10: Train Loss=1.7616, Val Acc=0.4988\n",
            "\n",
            "--- Transformer_layers2_heads2 ---\n",
            "Epoch 1/10: Train Loss=2.0076, Val Acc=0.4718\n",
            "Epoch 2/10: Train Loss=1.8140, Val Acc=0.4895\n",
            "Epoch 3/10: Train Loss=1.7701, Val Acc=0.4965\n",
            "Epoch 4/10: Train Loss=1.7473, Val Acc=0.5012\n",
            "Epoch 5/10: Train Loss=1.7310, Val Acc=0.5024\n",
            "Epoch 6/10: Train Loss=1.7210, Val Acc=0.5050\n",
            "Epoch 7/10: Train Loss=1.7121, Val Acc=0.5065\n",
            "Epoch 8/10: Train Loss=1.7061, Val Acc=0.5136\n",
            "Epoch 9/10: Train Loss=1.7008, Val Acc=0.5098\n",
            "Epoch 10/10: Train Loss=1.6947, Val Acc=0.5127\n",
            "\n",
            "--- Transformer_layers2_heads4 ---\n",
            "Epoch 1/10: Train Loss=1.9647, Val Acc=0.4781\n",
            "Epoch 2/10: Train Loss=1.7721, Val Acc=0.4972\n",
            "Epoch 3/10: Train Loss=1.7302, Val Acc=0.5034\n",
            "Epoch 4/10: Train Loss=1.7072, Val Acc=0.5102\n",
            "Epoch 5/10: Train Loss=1.6950, Val Acc=0.5113\n",
            "Epoch 6/10: Train Loss=1.6832, Val Acc=0.5155\n",
            "Epoch 7/10: Train Loss=1.6733, Val Acc=0.5165\n",
            "Epoch 8/10: Train Loss=1.6678, Val Acc=0.5179\n",
            "Epoch 9/10: Train Loss=1.6594, Val Acc=0.5185\n",
            "Epoch 10/10: Train Loss=1.6525, Val Acc=0.5220\n",
            "\n",
            "--- Transformer_layers4_heads2 ---\n",
            "Epoch 1/10: Train Loss=1.9905, Val Acc=0.4763\n",
            "Epoch 2/10: Train Loss=1.7927, Val Acc=0.4877\n",
            "Epoch 3/10: Train Loss=1.7441, Val Acc=0.4987\n",
            "Epoch 4/10: Train Loss=1.7130, Val Acc=0.5035\n",
            "Epoch 5/10: Train Loss=1.6910, Val Acc=0.5126\n",
            "Epoch 6/10: Train Loss=1.6733, Val Acc=0.5137\n",
            "Epoch 7/10: Train Loss=1.6610, Val Acc=0.5165\n",
            "Epoch 8/10: Train Loss=1.6531, Val Acc=0.5177\n",
            "Epoch 9/10: Train Loss=1.6454, Val Acc=0.5182\n",
            "Epoch 10/10: Train Loss=1.6386, Val Acc=0.5230\n",
            "\n",
            "--- Transformer_layers4_heads4 ---\n",
            "Epoch 1/10: Train Loss=1.9685, Val Acc=0.4807\n",
            "Epoch 2/10: Train Loss=1.7629, Val Acc=0.4918\n",
            "Epoch 3/10: Train Loss=1.7191, Val Acc=0.5058\n",
            "Epoch 4/10: Train Loss=1.6945, Val Acc=0.5086\n",
            "Epoch 5/10: Train Loss=1.6753, Val Acc=0.5120\n",
            "Epoch 6/10: Train Loss=1.6642, Val Acc=0.5172\n",
            "Epoch 7/10: Train Loss=1.6532, Val Acc=0.5193\n",
            "Epoch 8/10: Train Loss=1.6450, Val Acc=0.5202\n",
            "Epoch 9/10: Train Loss=1.6380, Val Acc=0.5212\n",
            "Epoch 10/10: Train Loss=1.6357, Val Acc=0.5187\n",
            "\n",
            "--- RNN Baseline (LSTM) ---\n",
            "Epoch 1/10: Train Loss=1.6618, Val Acc=0.5485\n",
            "Epoch 2/10: Train Loss=1.4424, Val Acc=0.5616\n",
            "Epoch 3/10: Train Loss=1.3877, Val Acc=0.5712\n",
            "Epoch 4/10: Train Loss=1.3562, Val Acc=0.5754\n"
          ]
        }
      ]
    }
  ]
}