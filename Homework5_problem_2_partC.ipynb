{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sesmael/Real-Time-ML-/blob/main/Homework5_problem_2_partC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL8KRQmNP28M"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# ---------------------------\n",
        "# Download and Process the Tiny Shakespeare Dataset\n",
        "# ---------------------------\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text  # The entire text data\n",
        "\n",
        "def process_data(sequence_length):\n",
        "    # Create a character mapping to integers\n",
        "    chars = sorted(list(set(text)))\n",
        "    char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "    int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    # Encode the text into integers\n",
        "    encoded_text = [char_to_int[ch] for ch in text]\n",
        "\n",
        "    # Create sequences and targets\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(encoded_text) - sequence_length):\n",
        "        seq = encoded_text[i:i + sequence_length]\n",
        "        target = encoded_text[i + sequence_length]\n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "    # Convert lists to PyTorch tensors\n",
        "    return torch.tensor(sequences, dtype=torch.long), torch.tensor(targets, dtype=torch.long), char_to_int, int_to_char\n",
        "\n",
        "# ---------------------------\n",
        "# Define the Dataset Class\n",
        "# ---------------------------\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ip6YympP28O",
        "outputId": "a699337d-c8c1-44aa-99f9-a00685b57489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "=== Sequence Length: 50 ===\n",
            "\n",
            "--- Transformer_layers1_heads2 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pcsal\\miniforge3\\envs\\py_cuda\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: Train Loss=2.0809, Val Acc=0.4488\n",
            "Epoch 2/10: Train Loss=1.9106, Val Acc=0.4639\n",
            "Epoch 3/10: Train Loss=1.8796, Val Acc=0.4657\n",
            "Epoch 4/10: Train Loss=1.8644, Val Acc=0.4737\n",
            "Epoch 5/10: Train Loss=1.8555, Val Acc=0.4751\n",
            "Epoch 6/10: Train Loss=1.8518, Val Acc=0.4777\n",
            "Epoch 7/10: Train Loss=1.8496, Val Acc=0.4730\n",
            "Epoch 8/10: Train Loss=1.8426, Val Acc=0.4754\n",
            "Epoch 9/10: Train Loss=1.8386, Val Acc=0.4798\n",
            "Epoch 10/10: Train Loss=1.8358, Val Acc=0.4790\n",
            "\n",
            "--- Transformer_layers1_heads4 ---\n",
            "Epoch 1/10: Train Loss=2.0398, Val Acc=0.4643\n",
            "Epoch 2/10: Train Loss=1.8471, Val Acc=0.4822\n",
            "Epoch 3/10: Train Loss=1.8078, Val Acc=0.4866\n",
            "Epoch 4/10: Train Loss=1.7887, Val Acc=0.4908\n",
            "Epoch 5/10: Train Loss=1.7759, Val Acc=0.4953\n",
            "Epoch 6/10: Train Loss=1.7675, Val Acc=0.4958\n",
            "Epoch 7/10: Train Loss=1.7609, Val Acc=0.4973\n",
            "Epoch 8/10: Train Loss=1.7575, Val Acc=0.4984\n",
            "Epoch 9/10: Train Loss=1.7539, Val Acc=0.4983\n",
            "Epoch 10/10: Train Loss=1.7549, Val Acc=0.4980\n",
            "\n",
            "--- Transformer_layers2_heads2 ---\n",
            "Epoch 1/10: Train Loss=1.9991, Val Acc=0.4731\n",
            "Epoch 2/10: Train Loss=1.7979, Val Acc=0.4891\n",
            "Epoch 3/10: Train Loss=1.7520, Val Acc=0.4907\n",
            "Epoch 4/10: Train Loss=1.7296, Val Acc=0.5043\n",
            "Epoch 5/10: Train Loss=1.7123, Val Acc=0.5055\n",
            "Epoch 6/10: Train Loss=1.6996, Val Acc=0.5035\n",
            "Epoch 7/10: Train Loss=1.6901, Val Acc=0.5085\n",
            "Epoch 8/10: Train Loss=1.6813, Val Acc=0.5108\n",
            "Epoch 9/10: Train Loss=1.6763, Val Acc=0.5122\n",
            "Epoch 10/10: Train Loss=1.6720, Val Acc=0.5123\n",
            "\n",
            "--- Transformer_layers2_heads4 ---\n",
            "Epoch 1/10: Train Loss=1.9708, Val Acc=0.4762\n",
            "Epoch 2/10: Train Loss=1.7664, Val Acc=0.4976\n",
            "Epoch 3/10: Train Loss=1.7235, Val Acc=0.5044\n",
            "Epoch 4/10: Train Loss=1.6976, Val Acc=0.5085\n",
            "Epoch 5/10: Train Loss=1.6820, Val Acc=0.5123\n",
            "Epoch 6/10: Train Loss=1.6706, Val Acc=0.5123\n",
            "Epoch 7/10: Train Loss=1.6610, Val Acc=0.5173\n",
            "Epoch 8/10: Train Loss=1.6525, Val Acc=0.5181\n",
            "Epoch 9/10: Train Loss=1.6446, Val Acc=0.5199\n",
            "Epoch 10/10: Train Loss=1.6385, Val Acc=0.5222\n",
            "\n",
            "--- Transformer_layers4_heads2 ---\n",
            "Epoch 1/10: Train Loss=1.9827, Val Acc=0.4789\n",
            "Epoch 2/10: Train Loss=1.7667, Val Acc=0.4921\n",
            "Epoch 3/10: Train Loss=1.7201, Val Acc=0.4995\n",
            "Epoch 4/10: Train Loss=1.6941, Val Acc=0.5071\n",
            "Epoch 5/10: Train Loss=1.6745, Val Acc=0.5112\n",
            "Epoch 6/10: Train Loss=1.6616, Val Acc=0.5111\n",
            "Epoch 7/10: Train Loss=1.6493, Val Acc=0.5154\n",
            "Epoch 8/10: Train Loss=1.6409, Val Acc=0.5170\n",
            "Epoch 9/10: Train Loss=1.6411, Val Acc=0.5204\n",
            "Epoch 10/10: Train Loss=1.6251, Val Acc=0.5250\n",
            "\n",
            "--- Transformer_layers4_heads4 ---\n",
            "Epoch 1/10: Train Loss=1.9611, Val Acc=0.4840\n",
            "Epoch 2/10: Train Loss=1.7504, Val Acc=0.4993\n",
            "Epoch 3/10: Train Loss=1.7079, Val Acc=0.5045\n",
            "Epoch 4/10: Train Loss=1.6787, Val Acc=0.5090\n",
            "Epoch 5/10: Train Loss=1.6608, Val Acc=0.5120\n",
            "Epoch 6/10: Train Loss=1.6465, Val Acc=0.5185\n",
            "Epoch 7/10: Train Loss=1.6340, Val Acc=0.5198\n",
            "Epoch 8/10: Train Loss=1.6217, Val Acc=0.5252\n",
            "Epoch 9/10: Train Loss=1.6155, Val Acc=0.5246\n",
            "Epoch 10/10: Train Loss=1.6089, Val Acc=0.5236\n",
            "\n",
            "--- RNN Baseline (LSTM) ---\n",
            "Epoch 1/10: Train Loss=1.6563, Val Acc=0.5460\n",
            "Epoch 2/10: Train Loss=1.4344, Val Acc=0.5664\n",
            "Epoch 3/10: Train Loss=1.3771, Val Acc=0.5740\n",
            "Epoch 4/10: Train Loss=1.3443, Val Acc=0.5776\n",
            "Epoch 5/10: Train Loss=1.3233, Val Acc=0.5822\n",
            "Epoch 6/10: Train Loss=1.3068, Val Acc=0.5827\n",
            "Epoch 7/10: Train Loss=1.2954, Val Acc=0.5846\n",
            "Epoch 8/10: Train Loss=1.2859, Val Acc=0.5861\n",
            "Epoch 9/10: Train Loss=1.2774, Val Acc=0.5867\n",
            "Epoch 10/10: Train Loss=1.2718, Val Acc=0.5873\n",
            "\n",
            "\n",
            "--- Experiment Results ---\n",
            "Transformer_layers1_heads2: Train Loss = 1.8358, Val Acc = 0.4790, Time = 749.52 sec, Model Size = 155585 parameters\n",
            "Transformer_layers1_heads4: Train Loss = 1.7549, Val Acc = 0.4980, Time = 647.89 sec, Model Size = 155585 parameters\n",
            "Transformer_layers2_heads2: Train Loss = 1.6720, Val Acc = 0.5123, Time = 990.77 sec, Model Size = 288065 parameters\n",
            "Transformer_layers2_heads4: Train Loss = 1.6385, Val Acc = 0.5222, Time = 1060.59 sec, Model Size = 288065 parameters\n",
            "Transformer_layers4_heads2: Train Loss = 1.6251, Val Acc = 0.5250, Time = 2045.70 sec, Model Size = 553025 parameters\n",
            "Transformer_layers4_heads4: Train Loss = 1.6089, Val Acc = 0.5236, Time = 1891.63 sec, Model Size = 553025 parameters\n",
            "RNN_Baseline: Train Loss = 1.2718, Val Acc = 0.5873, Time = 834.52 sec, Model Size = 420289 parameters\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# Define the Transformer Model for Next Character Prediction\n",
        "# ---------------------------\n",
        "class TransformerCharModel(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, embed_dim=128, num_layers=2, num_heads=2, hidden_dim=256, dropout=0.1):\n",
        "        \"\"\"\n",
        "        vocab_size: Number of unique characters\n",
        "        seq_len: Input sequence length\n",
        "        embed_dim: Dimension of character embeddings\n",
        "        num_layers: Number of transformer encoder layers\n",
        "        num_heads: Number of attention heads\n",
        "        hidden_dim: Hidden dimension of the feed-forward network\n",
        "        dropout: Dropout probability\n",
        "        \"\"\"\n",
        "        super(TransformerCharModel, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # Learned positional embeddings\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len)\n",
        "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
        "        x = x + self.pos_embedding  # add positional embeddings\n",
        "        x = self.transformer_encoder(x)  # (batch_size, seq_len, embed_dim)\n",
        "        # Use the output of the last time step for prediction\n",
        "        out = self.fc_out(x[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# ---------------------------\n",
        "# Define an RNN Baseline (LSTM) for Comparison\n",
        "# ---------------------------\n",
        "class LSTMCharModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1, dropout=0.0):\n",
        "        super(LSTMCharModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc_out(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# ---------------------------\n",
        "# Training and Evaluation Functions\n",
        "# ---------------------------\n",
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            correct += (predictions == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# ---------------------------\n",
        "# Main Experiment Loop (Only for Sequence Length = 50)\n",
        "# ---------------------------\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # We only run the experiment for sequence length 50\n",
        "    seq_len = 50\n",
        "    print(f\"\\n=== Sequence Length: {seq_len} ===\")\n",
        "\n",
        "    # Process data for sequence length 50\n",
        "    sequences, targets, char_to_int, int_to_char = process_data(seq_len)\n",
        "    dataset = CharDataset(sequences, targets)\n",
        "\n",
        "    # Simple train-validation split (80/20)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    vocab_size = len(char_to_int)\n",
        "\n",
        "    # We create a dictionary to store the experiment results\n",
        "    experiment_results = {}\n",
        "\n",
        "    # Hyperparameter grid for transformers\n",
        "    layer_options = [1, 2, 4]\n",
        "    head_options = [2, 4]\n",
        "\n",
        "    num_epochs = 10\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    # Loop over transformer hyperparameters combinations\n",
        "    for num_layers in layer_options:\n",
        "        for num_heads in head_options:\n",
        "            config_name = f\"Transformer_layers{num_layers}_heads{num_heads}\"\n",
        "            print(f\"\\n--- {config_name} ---\")\n",
        "            model = TransformerCharModel(\n",
        "                vocab_size,\n",
        "                seq_len=seq_len,\n",
        "                embed_dim=128,\n",
        "                num_layers=num_layers,\n",
        "                num_heads=num_heads,\n",
        "                hidden_dim=256,\n",
        "                dropout=0.1\n",
        "            ).to(device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            start_time = time.time()\n",
        "            for epoch in range(num_epochs):\n",
        "                train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
        "                val_acc = evaluate_model(model, val_loader, device)\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "            elapsed_time = time.time() - start_time\n",
        "            final_val_acc = evaluate_model(model, val_loader, device)\n",
        "\n",
        "            # Save results for this configuration\n",
        "            experiment_results[config_name] = {\n",
        "                \"Final_Train_Loss\": train_loss,\n",
        "                \"Final_Val_Accuracy\": final_val_acc,\n",
        "                \"Training_Time_sec\": elapsed_time,\n",
        "                \"Model_Size\": sum(p.numel() for p in model.parameters())\n",
        "            }\n",
        "\n",
        "    # Train and evaluate the RNN baseline (for comparison)\n",
        "    print(\"\\n--- RNN Baseline (LSTM) ---\")\n",
        "    rnn_model = LSTMCharModel(\n",
        "        vocab_size,\n",
        "        embed_dim=128,\n",
        "        hidden_dim=256,\n",
        "        num_layers=1,\n",
        "        dropout=0.0\n",
        "    ).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_model(rnn_model, train_loader, criterion, optimizer, device)\n",
        "        val_acc = evaluate_model(rnn_model, val_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "    elapsed_time = time.time() - start_time\n",
        "    final_val_acc = evaluate_model(rnn_model, val_loader, device)\n",
        "\n",
        "    experiment_results[\"RNN_Baseline\"] = {\n",
        "        \"Final_Train_Loss\": train_loss,\n",
        "        \"Final_Val_Accuracy\": final_val_acc,\n",
        "        \"Training_Time_sec\": elapsed_time,\n",
        "        \"Model_Size\": sum(p.numel() for p in rnn_model.parameters())\n",
        "    }\n",
        "\n",
        "    print(\"\\n\\n--- Experiment Results ---\")\n",
        "    for config, metrics in experiment_results.items():\n",
        "        print(f\"{config}: Train Loss = {metrics['Final_Train_Loss']:.4f}, \"\n",
        "              f\"Val Acc = {metrics['Final_Val_Accuracy']:.4f}, \"\n",
        "              f\"Time = {metrics['Training_Time_sec']:.2f} sec, \"\n",
        "              f\"Model Size = {metrics['Model_Size']} parameters\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py_cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}